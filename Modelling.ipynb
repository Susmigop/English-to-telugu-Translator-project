{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "TphUnHWf_52Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eff69e1d-0b6c-416b-b405-bdc731c9e46f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: import-ipynb in /usr/local/lib/python3.7/dist-packages (0.1.4)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.3.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.5.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (0.2.5)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.10.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.11.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import-ipynb) (3.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import re\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import nltk.translate.bleu_score as bleu\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "#%run pre_processing.ipynb\n",
        "#import pre_processing\n",
        "!pip install import-ipynb\n",
        "import import_ipynb\n",
        "import pre_processing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "X4Opm0xU_7OX"
      },
      "outputs": [],
      "source": [
        "#Reading the dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/Pre_processing.csv')\n",
        "eng_tel=shuffle(df)\n",
        "#eng_tel.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng_tel.drop('Unnamed: 0', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "wQXTot33o-WT"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "uYYoa6mfAc0Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "196d86a7-09b3-40d4-f1a8-17b7f54e65eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 Telugu  \\\n",
              "1246  <start> ఆధునిక పార్లమెంటరీ ప్రజాస్వామ్య దేశాల ...   \n",
              "2096  <start> అతను మండుతున్న వ్యక్తి మరియు రాజుకు కూ...   \n",
              "4631                               <start> విమర్శ <end>   \n",
              "5163  <start> కమిటీ యొక్క విధులు భారత ప్రభుత్వంతో సం...   \n",
              "2053  <start> నేను యుఎస్ వెళ్ళే ముందు నేను చెప్పలేదు...   \n",
              "\n",
              "                                                English  \n",
              "1246  <start> some of the salient features in the fu...  \n",
              "2096  <start> he was a fiery man an was not afrai of...  \n",
              "4631                            <start> criticism <end>  \n",
              "5163  <start> the functions of the committee are to ...  \n",
              "2053  <start> i must say that before i went to the u...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ce518d8-ebbe-47e8-b0fd-5bdfa0c0e576\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Telugu</th>\n",
              "      <th>English</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>&lt;start&gt; ఆధునిక పార్లమెంటరీ ప్రజాస్వామ్య దేశాల ...</td>\n",
              "      <td>&lt;start&gt; some of the salient features in the fu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2096</th>\n",
              "      <td>&lt;start&gt; అతను మండుతున్న వ్యక్తి మరియు రాజుకు కూ...</td>\n",
              "      <td>&lt;start&gt; he was a fiery man an was not afrai of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4631</th>\n",
              "      <td>&lt;start&gt; విమర్శ &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; criticism &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5163</th>\n",
              "      <td>&lt;start&gt; కమిటీ యొక్క విధులు భారత ప్రభుత్వంతో సం...</td>\n",
              "      <td>&lt;start&gt; the functions of the committee are to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2053</th>\n",
              "      <td>&lt;start&gt; నేను యుఎస్ వెళ్ళే ముందు నేను చెప్పలేదు...</td>\n",
              "      <td>&lt;start&gt; i must say that before i went to the u...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ce518d8-ebbe-47e8-b0fd-5bdfa0c0e576')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ce518d8-ebbe-47e8-b0fd-5bdfa0c0e576 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ce518d8-ebbe-47e8-b0fd-5bdfa0c0e576');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ],
      "source": [
        "eng_tel.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokensation: "
      ],
      "metadata": {
        "id": "UQVgGvQhwxrX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "1zNS9JknEWfa"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post',maxlen=20,dtype='int32')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "4jQkH882Fs9x"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(eng_tel['English'].values)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(eng_tel['Telugu'].values)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "4-Wg24AOFO9s"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "lUvO_uSiGCLC"
      },
      "outputs": [],
      "source": [
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train_Test Spilt: "
      ],
      "metadata": {
        "id": "Daa9FjXIwsbg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "7G9J9aanGEnY"
      },
      "outputs": [],
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building Requriments:"
      ],
      "metadata": {
        "id": "5i9eB6zzw4bA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "r9oyMQjLGZ4x"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 16\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 128\n",
        "units = 1024\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "\n",
        "vocab_inp_size =len(inp_lang.word_index.keys())\n",
        "#print(vocab_inp_size)\n",
        "vocab_tar_size =len(targ_lang.word_index.keys())\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Glove Usage for English Vocabulary:\n"
      ],
      "metadata": {
        "id": "1KgFxfeyw9ln"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "AKD-3jidQtND"
      },
      "outputs": [],
      "source": [
        "# Global Vector Which is Glove Which give very word in 100 vectors\n",
        "\n",
        "embeddings_index = dict()\n",
        "f = open('/content/drive/MyDrive/glove.6B.100d.txt',encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    #print(word)\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "    #print(embeddings_index[word])\n",
        "f.close()\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_inp_size+1, 100))\n",
        "for word, i in inp_lang.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder:\n"
      ],
      "metadata": {
        "id": "R1AWOdgdxEmA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "I_-Bj6ENHE4I"
      },
      "outputs": [],
      "source": [
        "# Encoder Declaration: Declaring Encoder which we passing the vectors to the encoder here I am using GRU(Gated recurrent units)\n",
        "#Reference from Internet\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"embedding_layer_encoder\",trainable=False)\n",
        "        self.gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_activation='sigmoid', recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state = hidden)\n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder:"
      ],
      "metadata": {
        "id": "_9qf9B-oxHfd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "afDryQ94HdKt"
      },
      "outputs": [],
      "source": [
        "# Decoding  \n",
        "#Reference from Internet\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_activation='sigmoid', recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # Bahaudan Attention layer\n",
        "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "\n",
        "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "        \n",
        "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
        "        \n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        \n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        \n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.dec_units))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calling of Encoders and Decoders:"
      ],
      "metadata": {
        "id": "Zwx4IYeOxK5H"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "8oGe0qnwHyA5"
      },
      "outputs": [],
      "source": [
        "#tf.keras.backend.clear_session()\n",
        "\n",
        "encoder = Encoder(vocab_inp_size+1, 100, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size+1, embedding_dim, units, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function:\n"
      ],
      "metadata": {
        "id": "GSqMOFAnxTvj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "hV6fzW6vQzXd"
      },
      "outputs": [],
      "source": [
        "# Loss Function:\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction='none')\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Passing Value into the Function:\n"
      ],
      "metadata": {
        "id": "OWRAXmpPxhnS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "xf-qboRWQ4nC"
      },
      "outputs": [],
      "source": [
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "    encoder.get_layer('embedding_layer_encoder').set_weights([embedding_matrix])\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNTA90Y5UfeG",
        "outputId": "4a52af82-bd2f-40ea-cba0-82e363290756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 5.4194\n",
            "Epoch 1 Batch 100 Loss 4.4580\n",
            "Epoch 1 Batch 200 Loss 4.6898\n",
            "Epoch 1 Batch 300 Loss 3.6412\n",
            "Previous Loss 4.401186943054199\n",
            "Epoch 2 Batch 0 Loss 4.9686\n",
            "Epoch 2 Batch 100 Loss 5.6720\n",
            "Epoch 2 Batch 200 Loss 3.3125\n",
            "Epoch 2 Batch 300 Loss 3.5197\n",
            "Previous Loss 4.091197490692139\n",
            "Epoch 3 Batch 0 Loss 3.9615\n",
            "Epoch 3 Batch 100 Loss 4.8506\n",
            "Epoch 3 Batch 200 Loss 3.8877\n",
            "Epoch 3 Batch 300 Loss 4.5092\n",
            "Previous Loss 3.961367607116699\n",
            "Epoch 4 Batch 0 Loss 3.3436\n",
            "Epoch 4 Batch 100 Loss 3.2262\n",
            "Epoch 4 Batch 200 Loss 3.8686\n",
            "Epoch 4 Batch 300 Loss 3.8844\n",
            "Previous Loss 3.824436902999878\n",
            "Epoch 5 Batch 0 Loss 4.3109\n",
            "Epoch 5 Batch 100 Loss 4.1379\n",
            "Epoch 5 Batch 200 Loss 3.1820\n",
            "Epoch 5 Batch 300 Loss 4.1687\n",
            "Previous Loss 3.6572601795196533\n"
          ]
        }
      ],
      "source": [
        "EPOCHS =5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  #start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "  print(f'Previous Loss {total_loss/steps_per_epoch:}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "pbC4ikDexmfq"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def preprocess(text):\n",
        "    text = text.lower() # lower casing\n",
        "    exclude = set(string.punctuation) # Set of all special characters\n",
        "    text = re.sub(\"'\", '', text) # remove the quotation marks if any\n",
        "    text = ''.join(ch for ch in text if ch not in exclude)\n",
        "    text = text.strip()\n",
        "    text = re.sub(\" +\", \" \", text) # remove extra spaces\n",
        "    text = '<start> ' + text + ' <end>'\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main Function"
      ],
      "metadata": {
        "id": "sPMsaiFexpcy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "o0sehhJFjZhE"
      },
      "outputs": [],
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=20, padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result,attention_plot\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result,attention_plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing:"
      ],
      "metadata": {
        "id": "G2Vg9zHbxuU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "Z9pfZPH2Rl2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3a3489-2aba-4d91-c8ca-f3e89733877d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter something to translate:you\n",
            "Input sentence in english :  you\n",
            "Predicted sentence in telugu :  మీరు ధన్యవాదాలు <end> \n"
          ]
        }
      ],
      "source": [
        "input_sentence=input(\"Enter something to translate:\")\n",
        "print('Input sentence in english : ',input_sentence)\n",
        "predicted_output,attention_plot=evaluate(input_sentence)\n",
        "print('Predicted sentence in telugu : ',predicted_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pickle\n",
        "#file = open('list.pkl', 'wb')\n",
        "# dump information to the file\n",
        "\n",
        "#pickle.dump(evaluate(), file)\n",
        "# close the file\n",
        "#file.close()\n"
      ],
      "metadata": {
        "id": "da-3WJ1Lqle1"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "Cfj5vU7fDa-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07551dfc-58a7-4eec-fa65-841eb952ee0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['మ'], ['ీ'], ['ర'], ['ు'], [], ['ధ'], ['న'], ['్'], ['య'], ['వ'], ['ా'], ['ద'], ['ా'], ['ల'], ['ు'], [], ['<'], ['e'], ['n'], ['d'], ['>'], []]\n"
          ]
        }
      ],
      "source": [
        "l =predicted_output\n",
        "split_list =[i.split() for i in l]\n",
        "print(split_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "dwWy6ShrCWdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ebbf2e-59d7-4509-e3ed-e4a51a69b5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU score -> 0\n"
          ]
        }
      ],
      "source": [
        "#Bleu score to find accuracy of the model\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "candidate = 'మీరు'.split()\n",
        "print('BLEU score -> {}'.format(sentence_bleu(split_list, candidate)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qYFFHgBr79Lq"
      },
      "execution_count": 200,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}